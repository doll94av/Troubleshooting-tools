Summary

Tcpdump is a powerful tool that logs the packets of a network interface (or all interfaces) of the host it runs on. This can be intimidating with all of the options available, but we can leverage some of the options to help gain insight into what traffic is entering and leaving the pod. 




Pod to pod

In the examples below, I am using nginx that runs on port 80 and a generic dnsutils pod. The app pod can be created by running kubectl run frontend --image=nginx (this application runs on port 80) and the dnsutils pod can be created with the below YAML:

apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always




After both pods are running, we will need to install tcpdump in the frontend pod, that can be done with the following sequence of commands:

kubectl exec -it --tty frontend -- /bin/bash
apt-get update
apt-get install tcpdump -y




After the frontend pod has tcpdump available, we will need to install CURL into the dnsutils pod. That can be achieved with the below:

kubectl exec -it --tty dnsutils -- /bin/bash
apt-get update
apt-get install curl -y




Ok, we are finally ready to test! So, we have tcpdump, and we have curl; how do we slot this together? Well, the first step is opening a shell into both pods, you will need two terminal windows open for this:

kubectl exec -it --tty dnsutils -- /bin/bash
kubectl exec -it --tty <frontend> -- /bin/bash




After we have done that, let's start a packet capture inside of the frontend pod. This can be done with the below:

tcpdump -A -s 0 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'




This will grab HTTP traffic on port 3000 and dump the headers/responses. After doing this, we will need to generate requests to this pod on port 3000. Running a 'kubectl get po -o wide' will show the internal IP for the pod:

kubectl get po -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
dnsutils 1/1 Running 0 40m 192.168.191.197 ip-10-0-73-80.us-west-2.compute.internal <none> <none>
test 1/1 Running 0 44m 192.168.244.197 ip-10-0-95-194.us-west-2.compute.internal <none> <none>




After we have the internal IP of the frontend pod, let's hit some endpoints and analyze the output. The first thing we will do is curl the 'home page'. From inside of the dnsutils pod, let's do that by running curl <nginx>:80 after doing so let's inspect the output from the tcpdump in the frontend pod:

root@f:/app# tcpdump -A -s 0 'tcp port 3000 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
16:41:22.663538 IP ip-192-168-191-197.us-west-2.compute.internal.39004 > test.3000: Flags [P.], seq 1283633125:1283633209, ack 243492921, win 489, options [nop,nop,TS val 3236549990 ecr 3335655689], length 84
E...g\@.>..7.........\..L.....h9.....=.....
...f... GET / HTTP/1.1
User-Agent: curl/7.38.0
Host: 192.168.244.197:3000
Accept: */*


16:41:22.664594 IP test.3000 > ip-192-168-191-197.us-west-2.compute.internal.39004: Flags [P.], seq 1:867, ack 84, win 488, options [nop,nop,TS val 3335655691 ecr 3236549990], length 866
E...._@.@.i&...........\..h9L..9....9e.....
.......fHTTP/1.1 200 OK
X-Powered-By: Express
Accept-Ranges: bytes
Content-Type: text/html; charset=UTF-8
Content-Length: 591
ETag: W/"24f-HJqNBZnZ8D/opvajY24GfGGsS0M"
Vary: Accept-Encoding
Date: Wed, 21 Sep 2022 16:41:22 GMT
Connection: keep-alive
Keep-Alive: timeout=5

<removed HTML response>




There is a lot to unpack here, so let's start from the top. The first interesting thing is the first line we log:

ip-192-168-191-197.us-west-2.compute.internal.39004




This shows the IP of where the request to our frontend came from. If we recall, when we checked the frontend pods IP, the dnsutils pod had an IP of '192.168.191.197'. Great! We now know where this packet originated from. After the initial request is received, we also dump the response. Within the response, we have our response code as well as all of the headers:

....3.p.HTTP/1.1 200 OK
X-Powered-By: Express
Accept-Ranges: bytes
Content-Type: text/html; charset=UTF-8
Content-Length: 591
ETag: W/"24f-HJqNBZnZ8D/opvajY24GfGGsS0M"
Vary: Accept-Encoding
Date: Wed, 21 Sep 2022 16:46:52 GMT
Connection: keep-alive
Keep-Alive: timeout=5




We can see that our application gave a 200 response and the response size. Now, how does this look like a failure? Lets curl a non existent endpoint from our DNS pod with curl <frontendPodIP>:80/thisendpointisfake. Below is what we see:

16:51:34.578373 IP ip-192-168-191-197.us-west-2.compute.internal.39006 > test.3000: Flags [P.], seq 4060922577:4060922679, ack 3008279897, win 489, options [nop,nop,TS val 3237161905 ecr 3336267604], length 102
E...@.@.>............^.......N.Y....i].....
..+...gTGET /thisendpointisfake HTTP/1.1
User-Agent: curl/7.38.0
Host: 192.168.244.197:3000
Accept: */*


16:51:34.580735 IP test.3000 > ip-192-168-191-197.us-west-2.compute.internal.39006: Flags [P.], seq 1:309, ack 102, win 488, options [nop,nop,TS val 3336267607 ecr 3237161905], length 308
E..h).@.@..............^.N.Y...7....77.....
..gW..+.HTTP/1.1 500 Internal Server Error
X-Powered-By: Express
Date: Wed, 21 Sep 2022 16:51:34 GMT
Connection: keep-alive
Keep-Alive: timeout=5
Transfer-Encoding: chunked

7c
Proxy error: Could not proxy request /thisendpointisfake from 192.168.244.197:3000 to https://localhost:3001 (ECONNREFUSED).
0




The output is mostly the same; we can see our host IP and all of our headers. The important thing to note here is we still get our response headers on failure (along with some additional error information). This is an excellent example of pod -> pod communication, but is it practical? In Kubernetes, you don't call pods from other pods. What would this look like if we were to make the same request through a service? Well, let's take a look. The below YAML is a definition for a service that relates to our pod:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    run: frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80




As pods are ephemeral, this more closely mimics how applications should communicate in customer environments. So, lets the get clusterIP of our service:

kubectl get svc
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 94m
my-service ClusterIP 10.97.56.228 <none> 3000/TCP 13m




With this, we can follow the same procedure as when we were curling the frontend pod but slot in the cluster-ip of my-service. From inside of the dnsutils pod, let's CURL the home endpoint again but with the service IP. The output is the same in the dnsutils pod, but the message received is a bit different in our frontend pod:




Wh17:11:03.101243 IP ip-192-168-191-197.us-west-2.compute.internal.36280 > test.3000: Flags [P.], seq 2101320213:2101320294, ack 3573700330, win 489, options [nop,nop,TS val 869896709 ecr 3337436127], length 81
E...H.@.>...............}?....V............
3.....;.GET / HTTP/1.1
User-Agent: curl/7.38.0
Host: 10.97.56.228:80
Accept: */*




The big difference here is that the host header is now referencing the IP of our service. The good news is that in this case, the initial IP still references the pod we made the request from. Overall the scenario where this information is needed is niche. Still, in situations where customers have doubts about traffic entering/leaving their pods, this can serve as a starting point for investigation.




Node level tcpdump

This also will not work in AWS environments where you are connecting through session manager, there are a few bugs out there regarding this plus some amazon security that blocks this. The above applies in terms of setup; run both the frontend pod and dnsutils pods. The dnsutils pod will still need CURL, but we want to install tcpdump on our node. After deploying both, note the Node that the frontend pod is deployed to:

kubectl get po -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
dnsutils 1/1 Running 0 8m51s 192.168.223.158 ip-10-0-129-155.ec2.internal <none> <none>
test 1/1 Running 0 8m59s 192.168.223.157 ip-10-0-129-155.ec2.internal <none> <none>




In the above case, the pods are on the same node, but they are not required to be. As we gather packet information on the host level, we will need to filter down to the specific interface we are working with. This can be achieved with a bit of leg work. Firstly, we need to get the running containers on the node:

crictl ps -a
CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID
bf1ef81eae304 4db8afc88fa88 2 minutes ago Running dnsutils 0 f36fd84e3ff7a

339ac007ee3f1 ffc6273ba9753 2 minutes ago Running frontend 0 b138a7ff5301e
...



After we have the running containers, we can inspect our frontend container to find the PID associated with it:

crictl inspect 339ac007ee3f1 | grep pid
"pid": 31730,
"pid": 1
"type": "pid"




Now that we have the IP, how can we find the relevant interface? Well, we can run an ip addr specifically in the namespace (OS namespace, not Kubernetes) to get a list of all available interfaces. You will need to substitute in the PID of your container nsenter -t <pid> -n ip addr:

nsenter -t 31730 -n ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
      valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
      valid_lft forever preferred_lft forever
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if32: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default
    link/ether 0e:7f:68:37:21:2c brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 192.168.223.157/32 brd 192.168.223.157 scope global eth0
      valid_lft forever preferred_lft forever
    inet6 fe80::c7f:68ff:fe37:212c/64 scope link
      valid_lft forever preferred_lft forever




The important one here is the eth0 entry, specifically we want to note the if32 portion. Moving back to the default OS namespace, we can check the available interfaces:

ip addr 
....
32: cali1037a54e65e@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 4
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
      valid_lft forever preferred_lft forever




The more containers that are running, the longer the list will be, but we can see these are linked as if32 corresponds to the host interface 32, and on the host, the if4 corresponds to the 4 in the container's namespace. So now we know the interface to which the container traffic is being routed to. With this information, all we need to do is modify our tcpdump to only capture traffic on this interface. We can achieve this by adding -i cali1037a54e65e":

tcpdump -A -i cali1037a54e65e -s 0 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'




Ok! So we have a tcpdump running on the Node that our pod/container runs on. Let's generate some requests just as we did beforehand. Within the dnsutils pod, we can curl the internal IP of the frontend pod as we did before. The output of the tcpdump looks like the below:

[root@ip-10-0-129-155 centos]# tcpdump -A -i cali1037a54e65e -s 0 'tcp port 3000 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on cali1037a54e65e, link-type EN10MB (Ethernet), capture size 262144 bytes
19:52:14.297332 IP ip-192-168-223-158.ec2.internal.45324 > ip-192-168-223-157.ec2.internal.hbci: Flags [P.], seq 2071856245:2071856329, ack 3420247288, win 225, options [nop,nop,TS val 2581443 ecr 2581443], length 84
E...g.@.?..)............{~.u........A......
.'c..'c.GET / HTTP/1.1
User-Agent: curl/7.38.0
Host: 192.168.223.157:3000
Accept: */*


19:52:14.298421 IP ip-192-168-223-157.ec2.internal.hbci > ip-192-168-223-158.ec2.internal.45324: Flags [P.], seq 1:867, ack 84, win 224, options [nop,nop,TS val 2581444 ecr 2581443], length 866
E....<@.@.?.................{~......D......
.'c..'c.HTTP/1.1 200 OK
X-Powered-By: Express
Accept-Ranges: bytes
Content-Type: text/html; charset=UTF-8
Content-Length: 591
ETag: W/"24f-HJqNBZnZ8D/opvajY24GfGGsS0M"
Vary: Accept-Encoding
Date: Wed, 21 Sep 2022 19:52:14 GMT
Connection: keep-alive
Keep-Alive: timeout=5




This mimics what we saw beforehand. We can see in the output above 'ip-192-168-223-158.ec2.internal.45324 > ip-192-168-223-157.ec2.internal.hbci' the 158 IP is the dnsutils pod, and 157 is our Frontend pod, so this method will yield the same results as if we were operating inside of the pod. This requires a bit more effort to get all of the relevant information in place but is a more practical approach for actual customer environments.




Considerations

While daunting, tcpdump is flexible and should be able to be configured to capture whatever information the customer is looking for. The drawback is that in customer environments, this may not be so straightforward. They may require specific certs to sniff traffic or may have an application pod that doesn't support a shell or has access to install tcpdump. While these may be blockers, at least having a potential path to push them down should be helpful. While in most cases it's not the easiest thing to setup, they realistically should be able to build an image where we can get tcpdump installed if they are super pushy about validating traffic going to/coming from a pod. In general the node level approach is a bit more feasible but both serve as options to obtain detailed information on traffic entering and leaving a pod. As for tcpdump, each customer may want different information. The tool is flexible and can be filtered for different scenarios, the above examples should serve as a starting point for any testing  or tweaking that needs to be done.



Ephemeral debug containers

This will not be easily accessible in the short term (DKP 2.3 supports Kubernetes 1.23.X), but as it will be enabled by default in Kubernetes 1.25 I wanted to get some information on it as it will be helpful in the future. Ephemeral containers are containers that can be added to a pod on-the-fly.  This is useful in cases where customer applications are running images that are distroless or are unable to install tcpdump for one reason or another. This also allows us to streamline the steps to get tcpdump (or other network debugging tools) available in customer pods. You can run them with the following kubectl debug -it <desiredContainerName> --image=k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3 --target=<pod name>, when this feature is enabled it will attach a container with the image of our choice to the running pod. When this is enabled, we can follow the procedure above to get tcpdump running on the pod and continue debugging from there. Below is some documentation surrounding this feature:

https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#container-exec:~:text=kubectl%20debug%20%2Dit%20ephemeral%2Ddemo%20%2D%2Dimage%3Dbusybox%3A1.28%20%2D%2Dtarget%3Dephemeral%2Ddemo
https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/